{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRLpV5ZlWgyR"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# DAV 6150 Module 7: Regression Modeling for Categorical  Response Variables\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjYoLtkQWgyT"
      },
      "source": [
        "## Module 5 Assignment Review\n",
        "\n",
        "### A Simple Method for Calculating a ROC Curve\n",
        "\n",
        "A ROC curve is formulated by plotting the __true positive rate__ (TPR) (a.k.a., __sensitivity__) against the __false positive rate__ (FPR) (a.k.a, __1 - specificity)__ for a given classifier's actual classifications and scored probabilities.\n",
        "\n",
        "The FPR is represented by the X axis while the TPR is represented by the Y axis.\n",
        "\n",
        "__Remember__:  \n",
        "\n",
        "- The __true positive rate__ (TPR) is the proportion of actual \"positive\" classification values that are __correctly identified as \"positive\"__. This is also known as the __sensitivity__ of a classifier, i.e., TP / (TP + FN)\n",
        "\n",
        "\n",
        "- The __false positive rate__ (FPR) is the proportion of actual \"negative\" classification values that are __incorrectly identified as \"positive\"__. FPR is defined as FP / (FP + TN), which is the same as __1 - specificity__ (remember that specificity = TN/(TN + FP) )\n",
        "\n",
        "\n",
        "To derive the values required for a ROC plot of a model we __iterate through an ordered sequence of 'K' classification threshold values__ within the range of $(0 < threshold <= 1)$ (e.g., if 'K' == 100, our iterator would span .01 through 1.00 via .01 increments).\n",
        "\n",
        "While iterating through the threshold values, we __compare the scored probabilities against each iterative threshold value__ to derive + store TPR and FPR values across the full range of iterable threshold values. When finished, we have accumulated 'K' pairs of TPR and FPR values.\n",
        "\n",
        "Then, plot the resulting TPR and FPR metrics against one another on a 2-dimensional plane.\n",
        "\n",
        "#### A Python-based example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zEp_NRr-W-cm"
      },
      "outputs": [],
      "source": [
        "''' PLEASE NOTE THAT THIS PYTHON CODE IS SHOWN ONLY FOR PURPOSES OF EXPLAINING\n",
        "HOW TO APPROACH THE CREATION OF A ROC CURVE FOR A BINARY CLASSIFIER\n",
        "TO MAKE USE OF IT YOU WOULD NEED TO INCORPORATE IT WITHIN YOUR OWN CONTEXT\n",
        "\n",
        "'actual' is the class variable (the actual classification) from the data set\n",
        "'probability' is the scored.probability variable from the data set '''\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "import numpy as np\n",
        "#create list of thresholds\n",
        "thresholds = np.arange(0,1,0.01)\n",
        "\n",
        "#create list of true positive rates\n",
        "tpr_list=[]\n",
        "\n",
        "  #create list of false positive rates\n",
        "fpr_list=[]\n",
        "\n",
        "#create new list of predictions for every threshold\n",
        "for i in thresholds: #for every threshold\n",
        "  new_predicted=[] #create new list of predictions\n",
        "  for j in range(len(probability)): #for each observation\n",
        "  #compare threshold to probability and append new prediction\n",
        "    if probability[j] < i:\n",
        "      new_predicted.append(0)\n",
        "    else:\n",
        "      new_predicted.append(1)\n",
        "\n",
        "  #calculate the tpr\n",
        "  tpr = sensitivity(actual, pd.Series(new_predicted))\n",
        "  tpr_list.append(tpr) #append the true positive rate to the list\n",
        "\n",
        "  #calculate the fpr\n",
        "  fpr = 1 - specificity(actual, pd.Series(new_predicted))\n",
        "  fpr_list.append(fpr) #append the false positive rate to the list\n",
        "\n",
        "# ...then continue to formulate your code for plotting tpr_list vs. fpr_list\n",
        "# via the Python graphics tool of your choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78TAfsFLWgyU"
      },
      "source": [
        "# Module 7: Regression Modeling for Categorical Response Variables\n",
        "\n",
        "- __Binary Logistic Regression__: The response variable is a __binary categorical variable__, while the explanatory variables can be either continuous, discrete or binary (e.g., dummy variables created from the values of a categorical variable).\n",
        "\n",
        "\n",
        "- __Multinomial Logistic Regression (MLR)__: The response variable is a categorical variable having __more than two (2) possible values__, while the explanatory variables can be either continuous, discrete or binary. There are many types of MLR models, and the exact type of MLR model you choose to use will be dependent on whether your response variable is __ordinal__ or __nominal__ in nature.\n",
        "\n",
        "\n",
        "### Binary Logistic Regression\n",
        "\n",
        "In binary logistic regression, the response variable is __binary__, i.e., it contains data encoded as either '1' (e.g., 'True') or '0' (e.g., 'False'). In other words, logistic regression is used for __classification__ problems.\n",
        "\n",
        "Logistic regression finds the best fitting __mathematical model__ (not a simple linear equation) to describe the relationship between the binary response variable and the explanatory variable(s). The values it generates are the coefficients of a formula to predict a __logit transformation__ (aka \"__log odds\"__) which is the logarithm of the odds $p$ of the probability of the presence of the characteristic of interest.\n",
        "\n",
        "$logit(p) = ln(p/(1-p)) = b0 + b1x1 + b2x2 + .. + bnxn$\n",
        "\n",
        "where $p$ = the probability of the __presence__ of a characteristic,\n",
        "\n",
        "$(1-p)$ = the probability of the __abscence__ of a characteristic,\n",
        "\n",
        "$b0$ = is a constant value\n",
        "\n",
        "$b1, b2, ..,bn$ = the regression coefficients for the explanatory variables $x1, x2, .., xn$\n",
        "\n",
        "Effective binary logistic regression models contain little to no collinearity amongst the explanatory variables used.\n",
        "\n",
        "The explanatory variables should be linearly related to the log odds of the probability of the presence of the characteristic of interest (i.e., the value of the response variable)\n",
        "\n",
        "Logistic regression generally does not work well when applied to very small data sets. For observational studies, a minimum of __500 samples/observations__ is recommended.\n",
        "\n",
        "Further explanation is available here:\n",
        "\n",
        "https://www.statisticssolutions.com/what-is-logistic-regression/\n",
        "\n",
        "\n",
        "\n",
        "### Multinomial Logistic Regression\n",
        "\n",
        "In multinomial logistic regression (MLR), the response variable is __multiclass__, i.e., it has __more than 2 possible values__. Multinomial regression calculates the probability of any observation being assigned to each of the possible classes. Therefore, the regression model must perform at least $n-1$ calculations, where $n$ is the number of possible classifications for the response variable.\n",
        "\n",
        "Why $n-1$ calculations? If we have $n$ classes, we can determine the probability of the $n$th class by simply summing the probabilities for each of the other classifications and subtracting that sum from $1$, i.e.,\n",
        "\n",
        "### $P(Class_n) = 1 - ( P(Class_1) + P(Class_2) + ... + P(Class_{n-1}) ) $\n",
        "\n",
        "As with binary logistic regression, the coefficients of a MLR model predict the __log odds__ of the probability of any observation belonging to each of the $n$ classes.\n",
        "\n",
        "The probability of observing class $k$ out of $n$ total classes is:\n",
        "\n",
        "# $ P(y_i = k) = \\frac { e^{S_k} }  {e^{S_1} + e^{S_2} + ... + e^{S_n} } $\n",
        "\n",
        "(see https://towardsdatascience.com/understanding-logistic-regression-coefficients-7a719ebebd35)\n",
        "\n",
        "When performing multiple logistic regression, the algorithm assigns a predicted classification for a given observation based on __which of the possible classifications achieves the largest log odds value__.\n",
        "\n",
        "\n",
        "### How to interpret logistic regression model coefficients\n",
        "\n",
        "\"A one unit increase in explanatory variable $x$ (with all other explanatory variables held constant) increases the log-odds of response variable $y$ by the amount indicated by the coefficient of $x$.\"\n",
        "\n",
        "However, in practice it is very common to simply examine the __directionality__ of the coefficients, i.e., if a coefficient is __negative__, then the larger the value of the explanatory variable, the more it will tend to __decrease__ the magnitude of the response variable. Conversely, if a coefficient is __positive__, then the larger the value of the explanatory variable, the more it will tend to __increase__ the magnitude of the response variable.\n",
        "\n",
        "\n",
        "### How to convert log odds to a probability\n",
        "\n",
        "- __Step 1__: Convert the log odds value to odds via __exponentiation__, i.e., the antilog of a log value. Assuming the log odds value we've been given is a __natural log__, we apply an __exponential function__ to convert the log odds value to and odds value.\n",
        "\n",
        "\n",
        "- __Step 2__: Calculate the probability as follows:\n",
        "\n",
        "### $ Prob = \\frac {odds} {1 + odds} $\n",
        "\n",
        "http://www.pmean.com/13/predicted.html\n",
        "\n",
        "\n",
        "### Advantages of Logistic Regression\n",
        "\n",
        "- Relatively computationally efficient + easy to implement\n",
        "\n",
        "\n",
        "- Relatively easy to interpret (though not as easy to interpret as linear regression models)\n",
        "\n",
        "\n",
        "- Can be effective even if features have not been scaled to similar ranges\n",
        "\n",
        "\n",
        "- Due to its simplicity, logistic regression is widely used as a baseline when comparing the performance of other more complex classification methods (e.g., decision trees, random forest, KNN, neural networks, etc.)\n",
        "\n",
        "\n",
        "### Disadvantages of Logistic Regression\n",
        "\n",
        "- Due to its simplicity, performance can lag behind that of more complex classification methods\n",
        "\n",
        "\n",
        "- Requires a __linear__ relationship between the explanatory variables and the response variable since it relies on a linear decision space\n",
        "\n",
        "\n",
        "- Requires that explanatory variables are __very__ independent of one another + that they have a substantive relationship with the response variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ri3rfRtWgyV"
      },
      "source": [
        "### Evaluating Logistic Regression Model Performance\n",
        "\n",
        "In __Module 5__ we learned about a wide variety of classification model performance metrics, all of which can be used for purposes of evaluating the performance of any type of logistic regression model:\n",
        "\n",
        "- Confusion Matrices: True Positives (TP), False Positives (FP), True Negatives (TN), False Negatives\n",
        " (FN)\n",
        "\n",
        "\n",
        "- Accuracy\n",
        "\n",
        "\n",
        "- Precision\n",
        "\n",
        "\n",
        "- Recall / Sensitivity\n",
        "\n",
        "\n",
        "- Specificity\n",
        "\n",
        "\n",
        "- F1 Score\n",
        "\n",
        "\n",
        "- ROC: __ROC (Receiver Operating Characteristic) Curve__: Calculate by plotting the __true positive rate (TPR)__ against the __false positive rate (FPR)__; http://www.saedsayad.com/model_evaluation_c.htm. Plotting TPR vs. FPR for a series of classification models constructed using different thresholds for predicting classifications yields a curve within a two-dimensional plane.\n",
        "\n",
        "\n",
        "- AUC: In a ROC plot, AUC is determined by calculating the area in the plot that falls __below / to the right of__ the ROC curve.  A random classifier has an area under the curve of 0.5, while AUC for a perfect classifier is equal to 1. The higher the AUC score, the better the performance of a model.  AUC scores from different models can be compared against one another to help us determine which model has the best performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82i90Ne9WgyV"
      },
      "source": [
        "## Logistic Regression example with scikit-learn\n",
        "\n",
        "From the \"Python For Data Analysis\" textbook (from your DAV 5400 course): Use the 'Titanic' data set which contains passenger survival rate data for the ocean-going vessel of that name that sank in 1912.\n",
        "\n",
        "Our goal is to __try to predict whether or not a passenger would have been more likely than not to survive the sinking of the Titanic__ based on the data contained in the data set (i.e., build a __predictive model__).\n",
        "\n",
        "- First, we will fit a logistic regression model on a __training__ data set using the data set's __PClass__, __Age__, and __Sex__ attributes.\n",
        "\n",
        "\n",
        "- Then we evaluate the model using a __testing__ data set that is exclusive of the data contained in the __training__ data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf-TPViEWgyV",
        "outputId": "d42d810c-f47f-40d2-f430-cfd3211dfe3a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500   NaN        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "3      0            113803  53.1000  C123        S  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the LogisticRegression() function from sklearn's 'linear_model' sub-library\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# load the training + testing data sets for the Titanic data\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/titanic/train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/titanic/test.csv')\n",
        "\n",
        "# display the first four rows of the data set\n",
        "train[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0buebf7HWgyW",
        "outputId": "6b6adc4c-a52e-408a-b3d0-cbbd999d197a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>IsFemale</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>3</td>\n",
              "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>363272</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>2</td>\n",
              "      <td>Myles, Mr. Thomas Francis</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>240276</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>3</td>\n",
              "      <td>Wirz, Mr. Albert</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315154</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass                              Name     Sex   Age  SibSp  \\\n",
              "0          892       3                  Kelly, Mr. James    male  34.5      0   \n",
              "1          893       3  Wilkes, Mrs. James (Ellen Needs)  female  47.0      1   \n",
              "2          894       2         Myles, Mr. Thomas Francis    male  62.0      0   \n",
              "3          895       3                  Wirz, Mr. Albert    male  27.0      0   \n",
              "\n",
              "   Parch  Ticket    Fare Cabin Embarked  IsFemale  \n",
              "0      0  330911  7.8292   NaN        Q         0  \n",
              "1      0  363272  7.0000   NaN        S         1  \n",
              "2      0  240276  9.6875   NaN        Q         0  \n",
              "3      0  315154  8.6625   NaN        S         0  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# lets check the first four rows of the test data\n",
        "# Note the lack of a \"Survived\" indicator !!!\n",
        "test[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k_RKFgmWgyX",
        "outputId": "68a42832-2a3b-4860-d7d4-ad6dce1a4a85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(891, 12)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# how many records in the training data set?\n",
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qxwvWn_WgyX",
        "outputId": "32c6481f-fd62-49d7-f0f1-6197080253c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "342"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# how many people survived in the training set?\n",
        "train.Survived.values.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei4Qa86LWgyX",
        "outputId": "7cf2e9cb-33d2-4002-bcc5-dfc94e710e27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.3838383838383838"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# what percentage of the training set survived?\n",
        "train.Survived.values.sum() / train.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu5sAkpJWgyX"
      },
      "source": [
        "__NOTE__: Since we know that 38.3% of the people in the training set survived, we could achieve a training model accuracy of (1 - .383) = 61.7% by simply predicting \"Did not survive\" for each passenger. This metric is referred to as the __null error rate__.  When evaluating the performance of a binary logistic regression model, always check to see whether the accuracy you are attaining exceeds the __null error rate__. If not, your model is unlikely to be of any value.\n",
        "\n",
        "\n",
        "Missing data will generally prevent the fitting of a model via either __scikit-learn__ or __statsmodels__, so we must first check both the training and testing data for missing data values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21S7eRQnWgyX",
        "outputId": "6835bf68-97e4-4e25-f61c-45573f243700"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Survived         0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age            177\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             0\n",
              "Cabin          687\n",
              "Embarked         2\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the training data for null values\n",
        "train.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiE86GCPWgyY"
      },
      "source": [
        "Within the training data, the __Age__, __Cabin__ and __Embarked__ attributes all have missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PpYZDbWWgyY",
        "outputId": "59503b15-b261-4734-c8ad-974d2a96ba8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age             86\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             1\n",
              "Cabin          327\n",
              "Embarked         0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the test data for null values\n",
        "test.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an85ypUwWgyY"
      },
      "source": [
        "Within the testing data, the __Age__, __Fare__ and __Cabin__  attributes all have missing data.\n",
        "\n",
        "So if we want to use __Age__ as part of our predictive model, we need to somehow fill the missing __Age__ values. In this instance, the author of the PfDA text chooses to use the relatively simple process of filling the missing values with the __median__ Age value found within the __training__ data set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSfSz_z_WgyY"
      },
      "outputs": [],
      "source": [
        "# find the median 'Age' value within the training data set\n",
        "impute_value = train['Age'].median()\n",
        "\n",
        "# now fill the missing 'Age' values in both the training and testing data sets\n",
        "train['Age'] = train['Age'].fillna(impute_value)\n",
        "test['Age'] = test['Age'].fillna(impute_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cs_in0zWgyY"
      },
      "source": [
        "Next, __we create a dummy indicator for the 'Sex' categorical variable__: the new dummy variable 'IsFemale' contains a '1' if the 'Sex' value for a passenger is 'Female' and a '0' otherwise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo9fpIgfWgyY"
      },
      "outputs": [],
      "source": [
        "# create a dummy variable for the 'Sex' attribute in both the training and\n",
        "# testing data sets\n",
        "train['IsFemale'] = (train['Sex'] == 'female').astype(int)\n",
        "test['IsFemale'] = (test['Sex'] == 'female').astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-MwFWRgWgyZ"
      },
      "source": [
        "Now we are ready to define our predictive model using the attributes we want to use as explanatory variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GliED4lRWgyZ",
        "outputId": "838253a2-7e0f-4071-c5c0-0d0289ba4478"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 3.,  0., 22.],\n",
              "       [ 1.,  1., 38.],\n",
              "       [ 3.,  1., 26.],\n",
              "       [ 1.,  1., 35.],\n",
              "       [ 3.,  0., 35.]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define a vector containing the names of the attributes to use\n",
        "predictors = ['Pclass', 'IsFemale', 'Age']\n",
        "\n",
        "# create a subset of the training data using ONLY the selected explanatory variables\n",
        "X_train = train[predictors].values\n",
        "\n",
        "# create a subset of the testing data using ONLY the selected explanatory variables\n",
        "X_test = test[predictors].values\n",
        "\n",
        "# isolate the response indicator for the training data\n",
        "y_train = train['Survived'].values\n",
        "\n",
        "# sanity check on training data\n",
        "X_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdfA8hBGWgyZ",
        "outputId": "eeff21e9-5bd1-422b-cc25-378c44210f98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 1, 1, 0], dtype=int64)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sanity check on response indicator\n",
        "y_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXymjGMGWgyZ",
        "outputId": "8aa62d66-b1fc-4304-b97e-9e3f4d624e65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\James T\\Anaconda3\\envs\\DAV5400\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We're using the LogisticRegression() method for this model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# fit the model: X_train contains our explanatory variables while\n",
        "# y_train contains the response variable\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gzdUC6mWgyZ",
        "outputId": "a38aba8c-573d-4389-fdff-ce63e812e4f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7946127946127947"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate the accuracy of the model relative to the training data set\n",
        "model.score(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7cbE_F6WgyZ"
      },
      "source": [
        "Recall from above we calculated the __null error rate__ for our data to be 61.7%. The model we've generated has an accuracy score of 79.46%. As such, our model appears to be useful.\n",
        "\n",
        "__What can we learn by examining the regression model coefficients for the explanatory variables?__\n",
        "\n",
        "While the actual numeric values of the coefficients of a regression model are often very difficult to interpret, we can use the fact that a coefficient is either positive or negative to determine what effect of an __increase__ in the value of a given variable will have on the predicted value of the response variable.\n",
        "\n",
        "In this binary logistic regression example, the value of the response variable is the \"log odds\" of the binary outcome being either a '0' or '1' ('0' meaning the passenger was more likely to perish while '1' indicates the passenger was more likely to survive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvc7BUmIWgyZ",
        "outputId": "3777bb26-253d-4e68-f4ff-61ae81ce55e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Pclass', 'IsFemale', 'Age']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[-1.07373582,  2.52093733, -0.02864864]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# examine the model coefficients for the explanatory variables\n",
        "print(predictors)\n",
        "model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVV3PcguWgya"
      },
      "source": [
        "From the above we see that:\n",
        "\n",
        "- __Passenger Class__: An increase in the value of 'Passenger_Class' is associated with a __decreased__ likelihood of survival, i.e., passengers in lower classes of service were more likely to perish than were passengers in relatively higher classes of service.\n",
        "\n",
        "\n",
        "- __IsFemale__: Being female increased the likelihood of survival\n",
        "\n",
        "\n",
        "- __Age__: The older the passenger, the less likely they were to survive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLtHVbtdWgya",
        "outputId": "13797530-bc0b-460b-e31e-5dac40867b78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# generate predictions for the test data using our new model\n",
        "y_predict = model.predict(X_test)\n",
        "y_predict[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHdaBLLOWgya"
      },
      "source": [
        "The 'y_predict' array contains predictions that answer the question: \"__Was a given passenger more likely than not to have survived the sinking of the Titanic?__\"\n",
        "\n",
        "If we had the actual __survived/perished__ indicators for the test data, we could now check the performance of the model via a variety of error metrics (e.g., accuracy, specificity, precision, recall, AUC, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6ecV3DAWgya"
      },
      "source": [
        "### Case Study: Logistic Regression with scikit-learn\n",
        "\n",
        "https://nbviewer.jupyter.org/gist/justmarkham/6d5c061ca5aee67c4316471f8c2ae976"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkwYKjyDWgya"
      },
      "source": [
        "# Module 7 Assignment Guidelines / Requirements\n",
        "https://yu.instructure.com/courses/63488/assignments/324308?module_item_id=1157311"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
