{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B4OjLJLy1vc"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# DAV 6150 Module 6: Regression Modeling for Numeric Response Variables\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4 Assignment Review\n",
        "\n",
        "\n",
        "### How to approach this (or analogous) problem(s):\n",
        "\n",
        "1) __EDA__: Calculate correlation metrics amongst all variables; Look for potential collinearity amongst explanatory variables; Examine distributions + summary statistics of all variables + comment on their appearance; Plot relationships between each potential explanatory variable and the response variable - what kinds of preliminary predictive inferences can we derive? Identify missing data values; etc.\n",
        "\n",
        "2) __Data Preparation__: how to handle missing data values? If imputation is needed, which imputation approach is likely to be the most effective relative to the data at hand, e.g., use a predictive model? KNN? similarity? a mean/median/mode? etc. Note that use of a mean is typically a \"last resort\" when no other approach is practical. How to handle non-normal distributions? Do we need to somehow normalize our data? How to handle numeric attributes having vastly different value ranges? Should we standardize or re-scale our data via some other method? etc..\n",
        "\n",
        "3) __Dimensionality Reduction + Feature Selection__: Based on the results of your EDA, select the explanatory variables you believe are likely to be the most useful within your model(s). Exclude variables that are highly correlated with one another (i.e., \"collinear\"). Exclude variables that exhibit low variance. Consider excluding variables that exhibit little relation with the response variable. Do all of this __before__ use of PCA or recursive feature elimination or VIF's or p-value analysis.\n",
        "\n",
        "### What types of models were used? Some examples:\n",
        "\n",
        "- Apply PCA to continuous numeric data; select some number of PC's; use selected PC's as the basis of a regression model (excluding all categorical explanatory variables).\n",
        "\n",
        "\n",
        "- Apply PCA to continuous numeric data; select some number of PC's; use selected PC's + a subset categorical variables as the basis of a regression model. Refine model via use of backward and bi-directional selection and VIFs.\n",
        "\n",
        "\n",
        "- Use correlation thresholds, then recursive feature elimination (p-value analysis), to produce a model with a small number of statistically significant and variables\n",
        "\n",
        "\n",
        "## Suggestions for Future Work:\n",
        "\n",
        "### Make sure you are engaging with ALL of the assigned reading materials within a given Module\n",
        "\n",
        "It was quite obvious from the M4 Assignment submits that significant learning materials were not reviewed prior to beginning work on the M4 Assignment. Relying solely on the content of your Live Session Lecture Notes will not ensure your success in this course. As was discussed at the start of the semester, the Lecture Notes do not cover every aspect of the content of a Module, and just because something is not covered within the Lecture Notes does not mean it is not important or relevant. The assigned readings cover a wide range of concepts and also provide many Python-based examples of how to properly implement those concepts. As such, if you are not engaging with those materials, you are unlikely to absorb/master a great deal of relevant knowledge available to you within this course.\n",
        "\n",
        "### Use the results of your EDA as the starting point for all downstream work\n",
        "\n",
        "When asked to apply dimensionality reduction and/or feature selection methods to a data set, we should rely upon the results of our EDA work, i.e., our starting point should be the use of the correlation metrics and preliminary predictive inferences we've derived from the data.\n",
        "\n",
        "We should __NOT__ start by simply throwing all of the data we've been given into a backward selection process or PCA: why bother with an EDA if you are simply going to ignore its results? The results of a thorough EDA will typically allow us to construct our models in a much more efficient and effective manner than will simply throwing our hands up and arbitrarily forcing all of our data into a model.\n",
        "\n",
        "### Skewed distributions? Consider transforming them to improve model performance\n",
        "\n",
        "As we discussed in Module 3, many machine learning algorithms require that numeric data attributes conform to a relatively Gaussian distribution. Box-Cox Transforms ( https://www.statisticshowto.datasciencecentral.com/box-cox-transformation/) can be applied to many non-normally distributed numeric variables for purposes of transforming such distributions into dispersements that are more Gaussian in nature.\n",
        "\n",
        "\n",
        "### Avoid the use of Python-based tools that you don't fully understand\n",
        "\n",
        "While Python (and many other languages) often provide very simple + highly abstracted tools that enable the implementation of very complex concepts via a very small amount of Python code, we should avoid the use of tools we don't fully understand. Improper use of such highly abstracted tools without sufficient knowledge of their underlying algorithms can easily result in our work being compromised by inaccurate and/or irrelevant results/output while also requiring the computation of large amounts of potentially unnecessary calculations.\n",
        "\n",
        "\n",
        "### PCA\n",
        "\n",
        "__Can we apply PCA to categorical features that have been converted to nominal numeric values (e.g., via one-hot encoding or label enconding)?__\n",
        "\n",
        "The answer is __NO__, we should __NEVER__ apply PCA to __ANY__ type of categorical data, even if the categorical information has been converted to (or was always in) numerical format. __PCA__ is meant to be __applied to continuous variables__, for which it tries to maximize the variance (i.e, the squared deviations) of the data. The concept of squared deviations doesn't really exist when applied to binary or label encoded data.\n",
        "\n",
        "By contrast, __categorical data is measured on a nominal scale__ meaning that the category spacing has no interval/ratio meaning.\n",
        "\n",
        "So while you can obtain an output from a PCA algorithm based on numeric encodings of categorical inputs, the output is highly unlikely to have any relevant \"meaning\" (i.e., garbage in ... garbage out).\n",
        "\n",
        "\n",
        "__Prior to applying PCA to a set of continuous numeric data, should we remove features that appear to be highly correlated?__\n",
        "\n",
        "Retaining highly correlated features can cause PCA to __over-emphasize__ the contribution of the highly correlated variables within the principal components + potentially change the direction of the associated eigenvectors and/or the magnitude of the associated eigenvalues. Here's a link to a fairly good explanation of this phenomena: https://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca\n",
        "\n",
        "So the answer to the question is __YES__, we should attempt to remove features that appear to be highly correlated with one another prior to applying PCA to a set of continuous numeric data.\n",
        "\n",
        "\n",
        "__Create a plot of cumulative explained variance to help you decide how many principal components to retain__\n",
        "\n",
        "The PCA content within __Module 4__ provides examples of how to properly determine the number of principal components to retain. Examples are provided in both the __MLPR__ and __HOML__ textbooks.\n",
        "\n",
        "\n",
        "### Variance Inflation Factors\n",
        "\n",
        "Variance inflation factors (VIFs) are an __OUTPUT__ of __a series of regression models__. To calculate VIFs for a set of explanatory variables, we need to regress every explanatory variable against every other possible explanatory variable. That's $N * (N-1)$ regression models !!!\n",
        "\n",
        "Therefore, we should __NOT__ be using VIFs __before__ we've attempted to remove highly correlated explanatory variables from a data set. VIFs are appropriately derived __from the output of regression models we have constructed using the knowledge we've gained from our EDA work__. This avoids the use of many arbitrary + unnecessary calculations while also __contextualizing the VIFs relative to a model that has been constructed via a process informed inquiry__, as opposed to an arbitrary calculation of VIFs prior to the application of the domain knowledge we develop via an EDA process.\n",
        "\n",
        "By contrast, when __evaluating__ \"least squares\"-based regression models (e.g., linear regression models), we should assess VIF metrics even after we've eliminated all non-statistically significant variables to ensure that the remaining variables are not collinear."
      ],
      "metadata": {
        "id": "b1gzCMtQ0ayI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHxzIn7Ay1vg"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# Regression Modeling for Numeric Response Variables\n",
        "\n",
        "In general, the phrase __regression modeling__ refers to the process of estimating the strength of the relationship between one variable (the __\"Response\" (aka \"Dependent\") variable__) and one or more __\"Explanatory\" (aka \"Independent\") variables__.\n",
        "\n",
        "\n",
        "Regression modeling is widely used for purposes of __predicting the mean numerical value of the response variable__ when given specific values for the explanatory variables.\n",
        "\n",
        "\n",
        "There are many different types of regression models, and the characteristics of each vary depending on the type of response variable you are attempting to estimate. Some of the most widely used types of regression models for __numerical response variables__ include:\n",
        "\n",
        "- __Linear Regression__: Used for fitting a linear equation for a __continuous__ numeric response variable. The relationship between the response and explanatory variable(s) is assumed to be __linear__ in nature. The output is a __linear equation__. If there is only one explanatory variable, we use __Simple Linear Regression__. If there is more than one explanatory variable, we use __Multiple Linear Regression__.\n",
        "\n",
        "\n",
        "- __\"Count\" Regression__: The response variable contains __non-negative__ (i.e., x >= 0) __discrete__ numeric \"count\" values while the explanatory variables can be either binary, discrete or continuous. Commonly used regression techniques for modeling count data include __Poisson Regression__ (where the response variable __must__ have a Poisson distribution), __Negative Binomial Regression__ (where the response variable need not have a Poisson distribution), and __Zero-Inflated Negative Binomial Regression__ (used when the response variable contains an excessive number of 'zero' values).\n",
        "\n",
        "\n",
        "- __Polynomial Regression__: Used when a __non-linear__ relationship exists between a continuous response variable and explanatory variables that are either binary, discrete or continuous. The ouput is a __non-linear__ equation.\n",
        "\n",
        "\n",
        "\\*\\* Please note that there are __many__ other types of regression models (e.g., Lasso, Ridge, Partial Least Squares, etc.), and describing each of them is beyond the scope of this lecture.\\*\\*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn-a1mEAy1vg"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "Linear regression consists of finding the best fitting straight line through a set of data points. That line is called a __regression line__. An example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNbzETPUy1vg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AjeH2_ey1vh",
        "outputId": "15823bdc-97d0-44a0-b074-30005ae4b79d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# generate some random data to use for the plot\n",
        "x = np.random.randn(50) * 10\n",
        "\n",
        "y = np.random.randn(50) * 10\n",
        "\n",
        "# first define a Matplotlib figure\n",
        "plt.figure()\n",
        "\n",
        "# use Seaborn's regplot() function to plot the regression line for the\n",
        "sns.regplot(x, y)\n",
        "\n",
        "# give the plot a title using Matplotlib\n",
        "plt.title('Regression Plot');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTOCtOswy1vh"
      },
      "source": [
        "Simple linear regression equations take the form of\n",
        "$y' = bx + a$ where $y'$ is the predicted value of the response variable for a given value of the explanatory variable $x$, $b$ is the slope of the regression line, $x$ is the value of the explanatory variable and $a$ is a constant representing the y-intercept of the regression line.\n",
        "\n",
        "\n",
        "The regression line is created by using the known values of the explanatory variable $x$ and the known values of the response variable $y$ to find the slope $b$ and y-intercept $a$ of the line that minimizes the sum of the squared errors of the predicted values of $y'$.\n",
        "\n",
        "\n",
        "We calculate values for the slope $b$ and the y-intercept $a$ using the means and standard deviations of the known values of $x$ and $y$ as well as the correlation coefficient $r$ of $x$ and $y$:\n",
        "\n",
        "$b = r * stddev(y)/stddev(x) $\n",
        "\n",
        "$a = mean(y) - b(mean(x))$\n",
        "\n",
        "__NOTE:__ The Python modeling libraries we are discussing below do these calculations for you. The calculations are shown here simply to provide further context.\n",
        "\n",
        "A simple linear regression example:\n",
        "\n",
        "http://onlinestatbook.com/2/regression/intro.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktconx0vy1vh"
      },
      "source": [
        "## Multiple Linear Regression\n",
        "\n",
        "When using multiple explanatory variables, the regression line __cannot be visualized within a two-dimensional space__. However, it is still relatively easy to compute: the regression line is estimated using the formula\n",
        "\n",
        "$y' = a + b1x1 + b2x2 + .. +bnxn$\n",
        "\n",
        "\n",
        "where $y'$ is the predicted value for the response variable for given values of the explanatory variables x1, x2, .., xn, and the $b$ values represent the __independent contributions__ of the corresponding explanatory variable to the prediction of the response variable.  \n",
        "\n",
        "For those who desire further explanation of the underlying mathematics:\n",
        "\n",
        "http://faculty.cas.usf.edu/mbrannick/regression/Reg2IV.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdHScvlVy1vh"
      },
      "source": [
        "## Count Regression\n",
        "\n",
        "Unlike linear regression, __count regression__ is appropriate for models in which the response variable is a __non-negative integer__.  The response variable should follow either a __Poisson distribution__ (i.e., mean == variance) or an __overdispersed distribution__ (i.e., variance > mean).\n",
        "\n",
        "\n",
        "#### Why not just use some form of linear regression for estimating non-negative integers?\n",
        "\n",
        "-  The relationship between the explanatory variables and the non-negative integer response variable __are unlikely to be linear__.\n",
        "\n",
        "\n",
        "-  Linear regression can produce __negative floating point numbers__, which are clearly not valid if the response variable is known to be a non-negative integer.\n",
        "\n",
        "\n",
        "-  The distribution of the residuals from a linear regression model that has been applied to a non-negative integer response variable __will not be random__.\n",
        "\n",
        "\n",
        "### Poisson Regression\n",
        "\n",
        "If your data follows a Poisson distribution, __Poisson Regression__ can be an effective tool for estimating the non-negative integer response variable. The probability mass function (PMF) for a Poisson distribution is:\n",
        "\n",
        "# $p(X = k) =  \\frac{\\lambda^k e^{-\\lambda}} {k!}$\n",
        "\n",
        "where $k$ is any non-negative integer, $e$ is the natural exponent $e = 2.71828$, and $\\lambda$ is the mean value of the response variable.  (Note that the mean is equivalent to the variance in a Poisson distribution). Using the PMF we can estimate the likelihood of any value within a Poisson distribution.\n",
        "\n",
        "Poisson regression incorporates this PMF as it calculates a __Maximum Likelihood Estimation__ for the regression coefficients that maximize the likelihoods of the individual response variable values.\n",
        "\n",
        "The output of the algorithm will be the __natural log__ of the count value you are attempting to estimate. Therefore, you must __exponentiate__ the output of the model to determine the actual estimated values for the count variable.\n",
        "\n",
        "__How to interpret the model's coefficients__: \"For a one unit change in $X_k$, the estimated count changes by a factor of $exp(\\beta_k)$, assuming all other variables are held constant.\" However, in practice it is very common to simply examine the __directionality__ of the coefficients, i.e., if a coefficient is __negative__, then the larger the value of the explanatory variable, the more it will tend to __decrease__ the magnitude of the response variable. Conversely, if a coefficient is __positive__, then the larger the value of the explanatory variable, the more it will tend to __increase__ the magnitude of the response variable.\n",
        "\n",
        "\n",
        "See this article from the assigned readings for more detail on the underlying mathematics + an example of how to implement Poisson regression in Python: https://towardsdatascience.com/an-illustrated-guide-to-the-poisson-regression-model-50cccba15958\n",
        "\n",
        "\n",
        "### Negative Binomial Regression\n",
        "\n",
        "If your data is __over dispersed__ (i.e, variance > mean), __Negative Binomial Regression__ can be an effective tool for estimating the non-negative integer response variable. The probability mass function (PMF) for a Negative Binomial (\"overdispersed\") distribution is:\n",
        "\n",
        "# $p(X = n) =  \\frac{(n-1)!} {(r-1)!(n-r)!}p^r (1-p)^{(n-r)}$\n",
        "\n",
        "where $n$ is the number of attempts, $r$ is the number of successes, and $p$ is the probability of success. For our purposes \"probability of success\" represents the likelihood that the response variable has been observed having a specific value (e.g., 0 or 1 or 2 or ...).\n",
        "\n",
        "Negative Binomial regression incorporates this PMF as it calculates a __Maximum Likelihood Estimation__ for the regression coefficients that maximize the likelihoods of the individual response variable values.\n",
        "\n",
        "As with Poisson regression, the output of the negative binomial regression algorithm will be the __natural log__ of the count value you are attempting to estimate. Therefore, you must __exponentiate__ the output of the model to determine the actual estimated value for the count variable.\n",
        "\n",
        "__How to interpret the model's coefficients__: Interpretation of the explanatory variable coefficients is identical to Poisson regression: \"For a one unit change in $X_k$, the estimated count changes by a factor of $exp(\\beta_k)$, assuming all other variables are held constant.\" However, in practice it is very common to simply examine the __directionality__ of the coefficients, i.e., if a coefficient is __negative__, then the larger the value of the explanatory variable, the more it will tend to __decrease__ the magnitude of the response variable. Conversely, if a coefficient is __positive__, then the larger the value of the explanatory variable, the more it will tend to __increase__ the magnitude of the response variable.\n",
        "\n",
        "See this article from the assigned readings for an explanation of how to implement negative binomial regression in Python: https://towardsdatascience.com/negative-binomial-regression-f99031bb25b4\n",
        "\n",
        "\n",
        "See this article from the assigned readings for an explanation of how to implement __both__ Poisson and Negative Binomial Regression via __Generalized Linear Models__ (GLM's: https://en.wikipedia.org/wiki/Generalized_linear_model) using Python's __statsmodels__ library:\n",
        "\n",
        "\n",
        "- https://dius.com.au/2017/08/03/using-statsmodels-glms-to-model-beverage-consumption/#cameron\n",
        "\n",
        "\n",
        "This article provides a detailed tutorial of how to implement both Poisson and Negative Binomial regression using Generalized Linear Models (GLM’s) via Python's statsmodels library. __DAV 6150 students should strongly consider making use of this tutorial when attempting to implement a count regression model within Python__.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-NZXQlwy1vi"
      },
      "source": [
        "## Polynomial Regression\n",
        "\n",
        "When a __non-linear__ relationship exists between a continuous response variable and explanatory variables that are either binary, discrete or continuous, a linear regression model is highly unlikely to be an effective model. As an alternative, we can substitute a __polynomial equation__ for the standard linear model. The ouput of such a regression model is a __non-linear__ equation.\n",
        "\n",
        "__What is a non-linear equation?__ An example:\n",
        "\n",
        "# $y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... + \\beta_dx_i^d + \\epsilon_i $\n",
        "\n",
        "As $d$ (the degree of the polynomial) increases, the \"flexibility\" of the regression model increases significantly. See the assigned reading for an example: https://towardsdatascience.com/polynomial-regression-bbe8b9d97491\n",
        "\n",
        "__HOWEVER__, this flexibility comes at a cost: we can easily end up with a model having very high variance (i.e., the model __overfits__ the data). A model with very high variance is unlikely to prove to be effective when applied to previously unseen data.\n",
        "\n",
        "__THEREFORE__: when using polynomial regression we need to be acutely aware of the __bias vs variance__ tradeoff and work diligently to select a polynomial degree $d$ that strikes a balance between the two.\n",
        "\n",
        "__It is unusual to use a $d$ of more than 3 or 4__ since using larger values can easily result in overfitting. However, there is no strict limitation on the selection of a value for $d$ so use your empirical skills to help you decide on a value for $d$ that is best suited to the data you are working with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dRhOeT4y1vi"
      },
      "source": [
        "## Evaluating Regression Model Performance\n",
        "\n",
        "In __Module 5__ we discussed the following regression model performance metrics:\n",
        "\n",
        "- $R^2$: Often referred to as \"Goodness of fit\"; measures how much of the variation in the response variable $y'$ is explained by variation in the explanatory variable(s). In general, __the larger the value of $R^2$, the more accurate the model is__. However, relatively large values of $R^2$ can also be an indication of the model being \"overfit\" to the training data.\n",
        "\n",
        "\n",
        "- __Adjusted__ $R^2$: Increases if a new variable added to the model improves the fit of the model by more than would be expected by sheer chance. When comparing two models derived from the same data, models with __higher__ __Adjusted__ $R^2$ scores are preferable to those having relatively lower scores.\n",
        "\n",
        "\n",
        "- __AIC__: Akaike Information Criteria is a model selection metric that estimates the relative quality of a statistical model for a given set of data. When comparing two models derived from the same data, models with __lower__ __AIC__ scores are preferable to those having relatively higher scores.\n",
        "\n",
        "\n",
        "- __BIC__: Bayesian Information Criteria is another model selection metric that estimates the unexplained variation in the response variable relative to the given explanatory variables. __BIC__ also imposes a \"complexity\" penalty when the number of explanatory variables used is increased.  When comparing two models derived from the same data, models with __lower__ __BIC__ scores are preferable to those having relatively higher scores.\n",
        "\n",
        "\n",
        "- __F Statistic__: Indicates whether a significant amount of variance in the response variable $y'$ is explained by the model. When comparing two models derived from the same data, models with __higher__ __F Statistic__ scores are preferable to those having relatively lower scores.\n",
        "\n",
        "\n",
        "- __Log Likelihood__: A measure of how well a model fits the underlying data. When comparing two models derived from the same data, models with __higher__ __Log Likelihood__ scores are preferable to those having relatively lower scores.\n",
        "\n",
        "\n",
        "- __p values__: Measure the statistical significance of the explanatory variables in your model. While you are free to select the significance level on your own, most often 0.05 is used as the maximum bound for significance. As such, if any variable in your model is shown to have a __p value__ that exceeds 0.05, consider removing it from the model to see whether the fit/model selection metrics improve.\n",
        "\n",
        "\n",
        "- __Root Mean Squared Error (RMSE)__: Average distance of a sample from its observed value to its predicted value. We calculate the RMSE by finding the square root of the average of the squared values of a model's residual values. When comparing two models derived from the same data, models with __lower__ __RMSE__ scores are preferable to those having relatively higher RMSE scores.\n",
        "\n",
        "\n",
        "These metrics allow us to assess the efficacy of our regression models. Practitioners should examine such metrics whenever they are readily available (e.g., if the software being used generates them automatically). If such metrics are not readily available as output from a model you are testing, you should consider whether your work requires you to calculate them via some other method as a separate step during your model evaluation work.\n",
        "\n",
        "\n",
        "### Residual Plots & Examining the \"Normality\" of Residuals\n",
        "\n",
        "In addition to the metrics listed above, we can also make use of the __residuals__ of a regression model to help us determine whether or not our models are robust.\n",
        "\n",
        "A __residual__ is the difference between the actual value of a response variable for a given observation and the predicted response variable value for that same observation.\n",
        "\n",
        "In general, the residuals for __linear regression models__ should be __normally distributed__ and __homoscedastic__ (i.e., their distribution is random across all estimated values and no pattern is emminently obvious). We can assess the residuals of a linear regression model via various types of plots. From your assigned readings (MLPR Ch 15): https://github.com/mattharrison/ml_pocket_reference/blob/master/ch15.ipynb\n",
        "\n",
        "Note that non-linear models are unlikely to have normally distributed residuals; as such, testing for the normality of residuals of such models is unnecessary."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read MLPR, Ch. 14, [Regression](https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb) (pp. 191-197)\n",
        "MLPR, Ch. 14\n",
        "Citation: Harrison, M. (2019). Regression. In Machine Learning Pocket Reference: Working with Structured Data in Python. Sebastopol: O'Reilly Media.\n",
        "\n",
        "Read HOML, Ch. 4, [Training Models](https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb) (pp. 111-118)\n",
        "HOML, Ch. 4\n",
        "Citation : Géron, A. (2019). Training models. In Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd ed.). Sebastopol: O'Reilly Media."
      ],
      "metadata": {
        "id": "hyFOvrgvzITg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s5ldz-4y1vi"
      },
      "source": [
        "# Project 1 Guidelines / Requirements"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}